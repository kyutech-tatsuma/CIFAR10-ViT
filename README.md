# Vision Transformer(ViT)
このレポジトリはCIFAR-10データセットをVision Transformer(ViT)モデルを使って学習させるための日本語ベースのレポジトリです。プログラム内に日本語でコメントも付与しているので、アルゴリズムやその実装方法への理解を深めるためにも利用できます
## Vision Transformer(ViT)の概要
Vision Transformer (ViT) は、画像分類タスクのために設計されたトランスフォーマー・ベースのアーキテクチャです。このモデルは、画像を一連の小さなパッチに分割し、それらを自然言語処理で一般的なトランスフォーマーの入力トークンのように扱います。各パッチは、位置情報を含むトークンに変換され、その後、標準的なトランスフォーマーエンコーダに供給されます。ViTは、深い自己注意機構を活用して、これらのパッチ間の関係を効果的に学習し、画像のクラス分類を行います。

ViTは、従来の畳み込みニューラルネットワーク(CNN)ベースのモデルと比較して、より少ない計算資源で高い精度を達成することが知られています。このモデルは、特に大規模なデータセットでの学習において、顕著な性能を発揮します。
## プログラムの実行方法
1. 依存ライブラリのインストール：必要なライブラリをインストールします。
```
pip install -r requirements.txt
```
2. プログラムの実行
```
python train.py --epochs <学習回数> --learning_rate <学習率> --patience <早期終了パラメータ> --islearnrate_search <学習率探索> --seed <シード> --batch <バッチサイズ>
```
### 引数の説明
--epochs: エポック数（訓練の反復回数）

--learning_rate: 学習率（デフォルトは0.001）

--patience: 早期終了のための耐性回数（バリデーションロスが改善しない場合の訓練継続エポック数）

--islearnrate_search: 学習率の探索を行うかどうか（'true' または 'false'）

--seed: 乱数生成のためのシード

--batch: バッチサイズ
### プログラムの応用
このプログラムは、カスタムデータセットや異なるモデルアーキテクチャにも適用可能です。必要に応じて、モデルの構造やデータ処理の部分を変更することで、様々な画像分類タスクに対応できます。